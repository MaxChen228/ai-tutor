# app/services/ai_service.py

import os
import json
import random
from app.assets import EXAMPLE_SENTENCE_BANK

# æ¢ä»¶å¼å°å…¥ AI æœå‹™ - é¿å…éƒ¨ç½²æ™‚å¼·åˆ¶å®‰è£
openai = None
genai = None

try:
    import openai
except ImportError:
    print("è­¦å‘Š: openai å¥—ä»¶æœªå®‰è£ï¼ŒOpenAI åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

try:
    import google.generativeai as genai
except ImportError:
    print("è­¦å‘Š: google-generativeai å¥—ä»¶æœªå®‰è£ï¼ŒGemini åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

MONITOR_MODE = True

# --- æ¨¡å‹è¨­å®šèˆ‡ API åˆå§‹åŒ– ---

AVAILABLE_MODELS = {
    "gpt-4o": "openai/gpt-4o",
    "gemini-2.5-pro": "gemini/gemini-2.5-pro",
    "gemini-2.5-flash": "gemini/gemini-2.5-flash",
}
DEFAULT_GENERATION_MODEL = "gemini-2.5-pro"
DEFAULT_GRADING_MODEL = "gemini-2.5-flash"

# OpenAI åˆå§‹åŒ–
openai_client = None
if openai and os.environ.get("OPENAI_API_KEY"):
    try:
        openai_client = openai.OpenAI()
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
else:
    print("è­¦å‘Š: OpenAI æœªå•Ÿç”¨ï¼ˆå¥—ä»¶æœªå®‰è£æˆ– API KEY æœªè¨­å®šï¼‰")

# Gemini åˆå§‹åŒ–  
gemini_api_key = None
gemini_generation_config = None
if genai and os.environ.get("GEMINI_API_KEY"):
    try:
        gemini_api_key = os.environ.get("GEMINI_API_KEY")
        genai.configure(api_key=gemini_api_key)
        gemini_generation_config = genai.GenerationConfig(response_mime_type="application/json")
    except Exception as e:
        print(f"åˆå§‹åŒ– Gemini æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
else:
    print("è­¦å‘Š: Gemini æœªå•Ÿç”¨ï¼ˆå¥—ä»¶æœªå®‰è£æˆ– API KEY æœªè¨­å®šï¼‰")

# --- æ–‡æ³•åº«è®€å– ---
try:
    with open("app/grammar_patterns.json", "r", encoding="utf-8") as f:
        grammar_patterns = json.load(f)
    print(f"æˆåŠŸè¼‰å…¥ {len(grammar_patterns)} ç­†æ–‡æ³•å¥å‹ã€‚")
except FileNotFoundError:
    print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° `app/grammar_patterns.json` æª”æ¡ˆã€‚æ–°é¡Œç”ŸæˆåŠŸèƒ½å°‡å—é™ã€‚")
    grammar_patterns = []
except json.JSONDecodeError:
    print("éŒ¯èª¤ï¼š`app/grammar_patterns.json` æ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•è§£æã€‚")
    grammar_patterns = []


# --- ç§æœ‰å‡½å¼ ---

def _call_llm_api(system_prompt, user_prompt, model_name, default_model):
    model_name = model_name or default_model
    full_model_id = AVAILABLE_MODELS.get(model_name)
    if not full_model_id:
        print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°åç‚º '{model_name}' çš„æ¨¡å‹ï¼Œå°‡ä½¿ç”¨é è¨­æ¨¡å‹ '{default_model}'ã€‚")
        full_model_id = AVAILABLE_MODELS[default_model]
    
    provider, model_id = full_model_id.split('/', 1)
    print(f"[AI Service] æ­£åœ¨ä½¿ç”¨æ¨¡å‹: Provider={provider}, Model ID={model_id}")

    if provider == 'gemini':
        gemini_model = genai.GenerativeModel(model_id, generation_config=gemini_generation_config)
        full_prompt = system_prompt + "\n\n" + user_prompt
        if MONITOR_MODE:
            print("\n" + "="*20 + f" Gemini API INPUT (Model: {model_id}) " + "="*20)
            print(full_prompt)
            print("="*60 + "\n")
        response = gemini_model.generate_content(full_prompt)
        return json.loads(response.text)
    elif provider == 'openai':
        if MONITOR_MODE:
            print("\n" + "="*20 + f" OpenAI API INPUT (Model: {model_id}) " + "="*20)
            print("--- SYSTEM PROMPT ---\n" + system_prompt)
            print("\n--- USER PROMPT ---\n" + user_prompt)
            print("="*60 + "\n")
        response = openai_client.chat.completions.create(
            model=model_id,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ LLM ä¾›æ‡‰å•†: {provider}")

def _normalize_questions_output(response_data):
    if isinstance(response_data, dict) and "questions" in response_data and isinstance(response_data["questions"], list):
        return response_data["questions"]
    elif isinstance(response_data, list):
         return response_data
    elif isinstance(response_data, dict):
        for value in response_data.values():
            if isinstance(value, list):
                return value
    return []

# --- å…¬å…±å‡½å¼ ---

def generate_new_question_batch(num_new, difficulty, length, model_name=None):
    if not grammar_patterns:
        print("éŒ¯èª¤ï¼šæ–‡æ³•åº«ç‚ºç©ºæˆ–è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•ç”Ÿæˆæ–°é¡Œç›®ã€‚")
        return []
    try:
        selected_patterns = random.sample(grammar_patterns, k=min(num_new, len(grammar_patterns)))
    except ValueError:
        return []
    generated_questions = []
    for i, pattern_data in enumerate(selected_patterns):
        print(f"\n[AI Service] æ­£åœ¨ç‚ºç¬¬ {i+1}/{len(selected_patterns)} å€‹æ–°å¥å‹å‡ºé¡Œ...")
        print(f"  - ç›®æ¨™å¥å‹: {pattern_data.get('pattern', 'N/A')}")
        example_sentences = EXAMPLE_SENTENCE_BANK.get(length, {}).get(str(difficulty), [])
        example_sentences_str = "\n".join([f"- {s}" for s in example_sentences]) if example_sentences else "ç„¡é¢¨æ ¼åƒè€ƒç¯„ä¾‹"
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½è‹±æ–‡å‘½é¡Œå°ˆå®¶ï¼Œä½ çš„å”¯ä¸€ä»»å‹™æ˜¯æ ¹æ“šæˆ‘æä¾›çš„ä¸€å€‹ã€Œæ ¸å¿ƒå¥å‹ã€ï¼Œè¨­è¨ˆä¸€é¡Œé«˜å“è³ªçš„ä¸­æ–‡ç¿»è­¯é¡Œã€‚
        **æŒ‡ä»¤ä¸€ï¼šå¼·åˆ¶ä½¿ç”¨æ ¸å¿ƒå¥å‹**
        ä½ ã€Œå¿…é ˆã€åœç¹ä»¥ä¸‹é€™å€‹å¥å‹ä¾†å‡ºé¡Œã€‚é¡Œç›®çš„ç­”æ¡ˆå¿…é ˆç”¨åˆ°é€™å€‹å¥å‹ã€‚
        ---
        ã€æ ¸å¿ƒå¥å‹è³‡æ–™ã€‘
        - å¥å‹: `{pattern_data.get('pattern', 'ç„¡')}`
        - èªªæ˜: `{pattern_data.get('explanation', 'ç„¡')}`
        - ç¯„ä¾‹: `{pattern_data.get('example_zh', 'ç„¡')} -> {pattern_data.get('example_en', 'ç„¡')}`
        ---
        **æŒ‡ä»¤äºŒï¼šæ¨¡ä»¿é¢¨æ ¼**
        é¡Œç›®çš„é›£åº¦ã€é•·åº¦å’Œèªæ°£ï¼Œè«‹ç›¡é‡æ¨¡ä»¿ä¸‹æ–¹çš„ã€Œé¢¨æ ¼åƒè€ƒç¯„ä¾‹ã€ã€‚
        ---
        ã€é¢¨æ ¼åƒè€ƒç¯„ä¾‹ã€‘
        {example_sentences_str}
        ---
        **æŒ‡ä»¤ä¸‰ï¼šåš´æ ¼è¼¸å‡ºæ ¼å¼**
        ä½ çš„å›è¦†ã€Œå¿…é ˆã€æ˜¯ä¸€å€‹ JSON ç‰©ä»¶ï¼Œä¸”ã€Œåªèƒ½ã€åŒ…å«é€™å…©å€‹ keyï¼š
        1. `new_sentence`: (string) ä½ è¨­è¨ˆçš„ä¸­æ–‡ç¿»è­¯é¡Œç›®ã€‚
        2. `hint_text`: (string) æç¤ºæ–‡å­—ï¼Œå…§å®¹å¿…é ˆæ˜¯æ ¸å¿ƒå¥å‹çš„ã€Œå¥å‹ã€æœ¬èº«ï¼Œä¾‹å¦‚ `S + V + as...as...`ã€‚
        """
        user_prompt = "è«‹åš´æ ¼éµç…§ä½ çš„ä¸‰é …æ ¸å¿ƒæŒ‡ä»¤ï¼Œç‚ºæˆ‘ç”Ÿæˆä¸€é¡Œè€ƒé¡Œçš„ JSON ç‰©ä»¶ã€‚"
        try:
            response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
            if isinstance(response_data, dict) and 'new_sentence' in response_data and 'hint_text' in response_data:
                response_data['hint_text'] = pattern_data.get('pattern', response_data['hint_text'])
                generated_questions.append(response_data)
                print(f"  - æˆåŠŸç”Ÿæˆé¡Œç›®: \"{response_data['new_sentence']}\"")
            else:
                print(f"  - è­¦å‘Šï¼šAI å›å‚³æ ¼å¼ä¸ç¬¦ï¼Œå·²è·³éæ­¤é¡Œã€‚æ”¶åˆ°çš„è³‡æ–™: {response_data}")
        except Exception as e:
            print(f"ç‚ºå¥å‹ '{pattern_data.get('pattern')}' ç”Ÿæˆé¡Œç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    return generated_questions

def generate_question_batch(weak_points_str, num_questions, model_name=None):
    """ç‚ºè¤‡ç¿’é¡Œç”Ÿæˆå•é¡Œ"""
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡æ•™å­¸ AIï¼Œå°ˆé–€ç‚ºå­¸ç”Ÿçš„å¼±é»è¨­è¨ˆã€Œè¤‡ç¿’é¡Œã€ã€‚
    
    ä»¥ä¸‹æ˜¯å­¸ç”Ÿéœ€è¦è¤‡ç¿’çš„å¼±é»çŸ¥è­˜ï¼š
    {weak_points_str}
    
    è«‹ç‚ºæ¯å€‹å¼±é»è¨­è¨ˆä¸€å€‹ä¸­æ–‡ç¿»è­¯é¡Œç›®ï¼Œç”¨ä¾†æ¸¬è©¦å­¸ç”Ÿæ˜¯å¦å·²æŒæ¡è©²çŸ¥è­˜é»ã€‚
    
    è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼š
    å›å‚³ä¸€å€‹ JSON é™£åˆ—ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ï¼š
    - new_sentence: ä¸­æ–‡ç¿»è­¯é¡Œç›®
    - hint_text: è€ƒé»æç¤ºï¼ˆç°¡æ½”æè¿°è©²çŸ¥è­˜é»ï¼‰
    
    è«‹ç¢ºä¿é¡Œç›®è¨­è¨ˆåˆç†ï¼Œèƒ½æœ‰æ•ˆæª¢é©—å­¸ç”Ÿå°è©²çŸ¥è­˜é»çš„æŒæ¡ç¨‹åº¦ã€‚
    """
    
    user_prompt = f"è«‹ç‚ºé€™ {num_questions} å€‹å¼±é»çŸ¥è­˜è¨­è¨ˆå°æ‡‰çš„è¤‡ç¿’é¡Œç›®ã€‚"
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        return _normalize_questions_output(response_data)
    except Exception as e:
        print(f"ç”Ÿæˆè¤‡ç¿’é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def get_tutor_feedback(chinese_sentence, user_translation, review_context=None, hint_text=None, model_name=None):
    """æ‰¹æ”¹ä½¿ç”¨è€…ç­”æ¡ˆä¸¦æä¾›å›é¥‹ã€‚"""
    
    error_analysis_instructions = """
    4.  `error_analysis`: (array of objects) éŒ¯èª¤åˆ†ææ¸…å–®ã€‚å¦‚æœæ²’æœ‰ä»»ä½•éŒ¯èª¤ï¼Œè«‹å›å‚³ä¸€å€‹ç©ºæ¸…å–® `[]`ã€‚
        æ¸…å–®ä¸­çš„æ¯ä¸€å€‹ç‰©ä»¶éƒ½ã€Œå¿…é ˆã€åŒ…å«ä»¥ä¸‹ã€Œæ‰€æœ‰ã€æ¬„ä½ï¼š
        * `error_type_code`: (string) ã€æ¥µé‡è¦ã€‘ä½ ã€Œå¿…é ˆã€å¾ä»¥ä¸‹å››å€‹ä»£ç¢¼ä¸­ï¼Œé¸æ“‡ã€Œä¸€å€‹ã€æœ€è²¼åˆ‡çš„åˆ†é¡ï¼š
            - `A`: è©å½™èˆ‡ç‰‡èªéŒ¯èª¤ (ç”¨å­—ä¸ç•¶ã€æ­é…è©éŒ¯èª¤ã€ä»‹ä¿‚è©éŒ¯èª¤ç­‰)
            - `B`: èªæ³•çµæ§‹éŒ¯èª¤ (æ™‚æ…‹ã€ä¸»è¢«å‹•ã€è©æ€§ã€å¥å­çµæ§‹éŒ¯èª¤ç­‰)
            - `C`: èªæ„èˆ‡èªç”¨éŒ¯èª¤ (èªæ„ä¸é€šã€é‚è¼¯çŸ›ç›¾ã€ä¸­å¼è‹±æ–‡ã€èªæ°£ä¸ç•¶ç­‰)
            - `D`: æ‹¼å¯«èˆ‡æ ¼å¼éŒ¯èª¤ (æ‹¼å­—ã€å¤§å°å¯«ã€æ¨™é»ç¬¦è™Ÿç­‰åŸºæœ¬éŒ¯èª¤)
        * `key_point_summary`: (string) ã€æœ€é‡è¦çš„æ¬„ä½ã€‘ç‚ºé€™å€‹éŒ¯èª¤é»æç…‰ä¸€å€‹ã€ŒéŒ¯èª¤ç„¦é»ã€ï¼Œä¾‹å¦‚ã€Œ'in the other hand' æ‡‰ç‚º 'on the other hand'ã€ã€‚æ­¤æ¬„ä½ã€Œå¿…é ˆã€ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚
        * `original_phrase`: (string) å¾å­¸ç”Ÿç­”æ¡ˆä¸­ï¼Œç²¾ç¢ºåœ°æå–å‡ºã€ŒéŒ¯èª¤ã€çš„é‚£å€‹å–®å­—æˆ–ç‰‡èªã€‚
        * `correction`: (string) é‡å°è©²éŒ¯èª¤ç‰‡èªï¼Œæä¾›ã€Œæ­£ç¢ºã€çš„å¯«æ³•ã€‚
        * `explanation`: (string) ã€Œå¿…é ˆã€ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ï¼Œç°¡æ½”åœ°è§£é‡‹ç‚ºä»€éº¼é€™æ˜¯éŒ¯çš„ï¼Œä»¥åŠæ‡‰è©²å¦‚ä½•ä¿®æ­£ã€‚
        * `severity`: (string) éŒ¯èª¤åš´é‡ç¨‹åº¦ï¼Œ`major` æˆ– `minor`ã€‚
    """
    
    overall_suggestion_instruction = "3.  `overall_suggestion`: (string) åœ¨ç¶œåˆè€ƒé‡æ‰€æœ‰éŒ¯èª¤å¾Œï¼Œæä¾›ä¸€å€‹ã€Œæ•´é«”æœ€ä½³ã€çš„ç¿»è­¯å»ºè­°ã€‚é€™å€‹å»ºè­°ã€Œå¿…é ˆã€æ˜¯å®Œæ•´çš„å¥å­ï¼Œä¸”ã€Œå¿…é ˆã€ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚"

    if review_context:
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½åš´è¬¹çš„è‹±æ–‡æ•™å­¸è©•åˆ† AIã€‚
        **é¦–è¦ä»»å‹™**: åˆ¤æ–·å­¸ç”Ÿä½œç­”æ˜¯å¦æŒæ¡äº†ã€Œæ ¸å¿ƒè¤‡ç¿’è§€å¿µ: {review_context}ã€ï¼Œä¸¦åœ¨å›å‚³çš„ JSON ä¸­è¨­å®š `did_master_review_concept` (boolean) çš„å€¼ã€‚
        **æ¬¡è¦ä»»å‹™**: åˆ†æå…¶ä»–èˆ‡æ ¸å¿ƒè§€å¿µç„¡é—œçš„éŒ¯èª¤ã€‚
        **æ’ä»–æ€§åŸå‰‡**: `error_analysis` åˆ—è¡¨çµ•å°ä¸æ‡‰åŒ…å«èˆ‡æ ¸å¿ƒè¤‡ç¿’è§€å¿µç›¸é—œçš„åˆ†æã€‚
        **è¼¸å‡ºæ ¼å¼**: ä½ çš„ JSON å›è¦†å¿…é ˆåŒ…å«ä»¥ä¸‹æ‰€æœ‰æ¬„ä½ï¼š
        1.  `did_master_review_concept`: (boolean)
        2.  `is_generally_correct`: (boolean)
        {overall_suggestion_instruction}
        {error_analysis_instructions}
        
        **åŸå§‹ä¸­æ–‡å¥å­æ˜¯**: "{chinese_sentence}"
        """
    else:
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä½ç´°å¿ƒä¸”åš´è¬¹çš„è‹±æ–‡å®¶æ•™ AIã€‚
        **æ ¸å¿ƒè€ƒé»æç¤º**: ã€Œ{hint_text if hint_text else 'ç„¡ç‰¹å®šæç¤º'}ã€ã€‚è«‹ç‰¹åˆ¥ç•™æ„æ­¤é»çš„æŒæ¡æƒ…æ³ã€‚
        **è¼¸å‡ºæ ¼å¼**: ä½ çš„ JSON å›è¦†å¿…é ˆåŒ…å«ä»¥ä¸‹æ‰€æœ‰æ¬„ä½ï¼š
        1.  `is_generally_correct`: (boolean)
        2.  `overall_suggestion`: (string) åœ¨ç¶œåˆè€ƒé‡æ‰€æœ‰éŒ¯èª¤å¾Œï¼Œæä¾›ä¸€å€‹ã€Œæ•´é«”æœ€ä½³ã€çš„ç¿»è­¯å»ºè­°ã€‚é€™å€‹å»ºè­°ã€Œå¿…é ˆã€æ˜¯å®Œæ•´çš„å¥å­ï¼Œä¸”ã€Œå¿…é ˆã€ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚
        {error_analysis_instructions}

        **åŸå§‹ä¸­æ–‡å¥å­æ˜¯**: "{chinese_sentence}"
        """
        
    user_prompt = f"é€™æ˜¯å­¸ç”Ÿçš„ç¿»è­¯ï¼šã€Œ{user_translation}ã€ã€‚è«‹æ ¹æ“šä½ çš„å°ˆæ¥­çŸ¥è­˜å’Œä¸Šè¿°åš´æ ¼æŒ‡ä»¤ï¼Œç‚ºæˆ‘ç”Ÿæˆä¸€ä»½é‰…ç´°é¡éºã€å®Œå…¨ç¬¦åˆæ ¼å¼çš„ JSON åˆ†æå ±å‘Šã€‚"

    try:
        return _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GRADING_MODEL)
    except Exception as e:
        print(f"AI æ‰¹æ”¹æ™‚ç™¼ç”ŸéŒ¯èª¤ - Model: {model_name}, Error: {e}")
        return {
            "did_master_review_concept": False, 
            "is_generally_correct": False, 
            "overall_suggestion": f"AI æ¨¡å‹ ({model_name}) æš«æ™‚ç„¡æ³•æä¾›æœå‹™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
            "error_analysis": [{
                "error_type_code": "E",
                "key_point_summary": "ç³»çµ±éŒ¯èª¤",
                "original_phrase": "N/A",
                "correction": "N/A",
                "explanation": f"ç³»çµ±ç„¡æ³•è™•ç† AI çš„å›è¦†ï¼š{e}",
                "severity": "major"
            }]
        }

def merge_error_analyses(error1, error2):
    """ä½¿ç”¨ AI å°‡å…©å€‹éŒ¯èª¤åˆ†ææ™ºæ…§åœ°åˆä½µæˆä¸€å€‹ã€‚"""
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±æ–‡æ•™å­¸ AI åŠ©ç†ã€‚
    ä½ çš„ä»»å‹™æ˜¯å°‡å…©å€‹ç›¸é—œçš„è‹±æ–‡éŒ¯èª¤åˆ†æã€Œæ™ºæ…§åœ°åˆä½µã€æˆä¸€å€‹æ›´ç²¾ç…‰ã€æ›´æœ‰æ•™å­¸åƒ¹å€¼çš„çŸ¥è­˜é»ã€‚
    
    åˆä½µåŸå‰‡ï¼š
    1. æ‰¾å‡ºå…©å€‹éŒ¯èª¤çš„å…±åŒæ ¸å¿ƒè§€å¿µ
    2. æ•´åˆå®ƒå€‘çš„è§£é‡‹ï¼Œå»é™¤é‡è¤‡éƒ¨åˆ†
    3. ä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„éŒ¯èª¤ç‰‡èªä½œç‚ºç¯„ä¾‹
    4. ç”Ÿæˆä¸€å€‹æ¶µè“‹å…©è€…çš„æ–°æ¨™é¡Œï¼ˆkey_point_summaryï¼‰
    
    è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
    {
        "error_type_code": "A/B/C/D",
        "key_point_summary": "åˆä½µå¾Œçš„æ ¸å¿ƒè§€å¿µæ¨™é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
        "original_phrase": "ä»£è¡¨æ€§çš„éŒ¯èª¤ç‰‡èª",
        "correction": "æ­£ç¢ºçš„ç‰‡èª",
        "explanation": "æ•´åˆå¾Œçš„è§£é‡‹ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
        "severity": "major/minor"
    }
    """
    
    user_prompt = f"""
    è«‹å°‡ä»¥ä¸‹å…©å€‹è‹±æ–‡éŒ¯èª¤åˆ†æåˆä½µæˆä¸€å€‹ï¼š
    
    éŒ¯èª¤åˆ†æ 1ï¼š
    - åˆ†é¡ï¼š{error1.get('error_type_code', 'N/A')}
    - æ¨™é¡Œï¼š{error1.get('key_point_summary', 'N/A')}
    - éŒ¯èª¤ç‰‡èªï¼š"{error1.get('original_phrase', 'N/A')}"
    - æ­£ç¢ºç‰‡èªï¼š"{error1.get('correction', 'N/A')}"
    - è§£é‡‹ï¼š{error1.get('explanation', 'N/A')}
    - åš´é‡ç¨‹åº¦ï¼š{error1.get('severity', 'N/A')}
    
    éŒ¯èª¤åˆ†æ 2ï¼š
    - åˆ†é¡ï¼š{error2.get('error_type_code', 'N/A')}
    - æ¨™é¡Œï¼š{error2.get('key_point_summary', 'N/A')}
    - éŒ¯èª¤ç‰‡èªï¼š"{error2.get('original_phrase', 'N/A')}"
    - æ­£ç¢ºç‰‡èªï¼š"{error2.get('correction', 'N/A')}"
    - è§£é‡‹ï¼š{error2.get('explanation', 'N/A')}
    - åš´é‡ç¨‹åº¦ï¼š{error2.get('severity', 'N/A')}
    
    è«‹åˆ†æé€™å…©å€‹éŒ¯èª¤çš„é—œè¯æ€§ï¼Œä¸¦æ™ºæ…§åœ°åˆä½µæˆä¸€å€‹æ›´æœ‰åƒ¹å€¼çš„çŸ¥è­˜é»ã€‚
    """
    
    try:
        merged_data = _call_llm_api(system_prompt, user_prompt, None, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " AI åˆä½µçµæœ " + "="*20)
            print(json.dumps(merged_data, ensure_ascii=False, indent=2))
            print("="*50 + "\n")
        
        return merged_data
        
    except Exception as e:
        print(f"AI åˆä½µéŒ¯èª¤åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            "error_type_code": error1.get('error_type_code', 'A'),
            "key_point_summary": f"{error1.get('key_point_summary', '')} / {error2.get('key_point_summary', '')}",
            "original_phrase": error1.get('original_phrase', ''),
            "correction": error1.get('correction', ''),
            "explanation": f"{error1.get('explanation', '')} å¦å¤–ï¼Œ{error2.get('explanation', '')}",
            "severity": "major" if error1.get('severity') == 'major' or error2.get('severity') == 'major' else "minor"
        }

def ai_review_knowledge_point(knowledge_point_data, model_name=None):
    """
    AI é‡æ–°å¯©é–±çŸ¥è­˜é»ï¼Œæä¾›æ”¹é€²å»ºè­°ã€‚
    
    Args:
        knowledge_point_data (dict): çŸ¥è­˜é»çš„å®Œæ•´è³‡æ–™
        model_name (str): ä½¿ç”¨çš„æ¨¡å‹åç¨±
    
    Returns:
        dict: AI å¯©é–±çµæœ
    """
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±èªæ•™å­¸å°ˆå®¶å’ŒçŸ¥è­˜ç®¡ç†é¡§å•ã€‚
    ä½ çš„ä»»å‹™æ˜¯ä»”ç´°å¯©é–±ä¸€å€‹è‹±æ–‡å­¸ç¿’çŸ¥è­˜é»ï¼Œä¸¦æä¾›å»ºè¨­æ€§çš„æ”¹é€²å»ºè­°ã€‚
    
    å¯©é–±é‡é»ï¼š
    1. çŸ¥è­˜é»çš„æº–ç¢ºæ€§å’Œå®Œæ•´æ€§
    2. è§£é‡‹çš„æ¸…æ™°åº¦å’Œæ•™å­¸æ•ˆæœ
    3. éŒ¯èª¤åˆ†é¡æ˜¯å¦åˆé©
    4. æ˜¯å¦æœ‰éºæ¼çš„é‡è¦è³‡è¨Š
    5. å­¸ç¿’è€…å¯èƒ½çš„å›°æƒ‘é»
    
    è«‹æä¾›å…·é«”ã€å¯¦ç”¨çš„æ”¹é€²å»ºè­°ï¼Œä¸¦è©•ä¼°é€™å€‹çŸ¥è­˜é»çš„æ•´é«”å“è³ªã€‚
    
    è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
    {
        "overall_assessment": "æ•´é«”è©•ä¼°ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
        "accuracy_score": 1-10,
        "clarity_score": 1-10,
        "teaching_effectiveness": 1-10,
        "improvement_suggestions": [
            "å»ºè­°1ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
            "å»ºè­°2ï¼ˆç¹é«”ä¸­æ–‡ï¼‰"
        ],
        "potential_confusions": [
            "å¯èƒ½çš„å›°æƒ‘é»1ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
            "å¯èƒ½çš„å›°æƒ‘é»2ï¼ˆç¹é«”ä¸­æ–‡ï¼‰"
        ],
        "recommended_category": "å»ºè­°çš„éŒ¯èª¤åˆ†é¡ï¼ˆå¦‚æœéœ€è¦æ›´æ”¹ï¼‰",
        "additional_examples": [
            "é¡å¤–çš„ä¾‹å¥1",
            "é¡å¤–çš„ä¾‹å¥2"
        ]
    }
    """
    
    user_prompt = f"""
    è«‹å¯©é–±ä»¥ä¸‹è‹±æ–‡å­¸ç¿’çŸ¥è­˜é»ï¼š
    
    ã€çŸ¥è­˜é»è³‡æ–™ã€‘
    - åˆ†é¡ï¼š{knowledge_point_data.get('category', 'N/A')}
    - å­åˆ†é¡ï¼š{knowledge_point_data.get('subcategory', 'N/A')}
    - æ ¸å¿ƒè§€å¿µï¼š{knowledge_point_data.get('key_point_summary', 'N/A')}
    - æ­£ç¢ºç”¨æ³•ï¼š{knowledge_point_data.get('correct_phrase', 'N/A')}
    - éŒ¯èª¤ç”¨æ³•ï¼š{knowledge_point_data.get('incorrect_phrase_in_context', 'N/A')}
    - è§£é‡‹ï¼š{knowledge_point_data.get('explanation', 'N/A')}
    - å­¸ç¿’è€…åŸå¥ï¼š{knowledge_point_data.get('user_context_sentence', 'N/A')}
    - ç†Ÿç·´åº¦ï¼š{knowledge_point_data.get('mastery_level', 0)}/5.0
    - éŒ¯èª¤æ¬¡æ•¸ï¼š{knowledge_point_data.get('mistake_count', 0)}
    - ç­”å°æ¬¡æ•¸ï¼š{knowledge_point_data.get('correct_count', 0)}
    
    è«‹æä¾›è©³ç´°çš„å¯©é–±å ±å‘Šå’Œæ”¹é€²å»ºè­°ã€‚
    """
    
    try:
        review_result = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " AI å¯©é–±çµæœ " + "="*20)
            print(json.dumps(review_result, ensure_ascii=False, indent=2))
            print("="*50 + "\n")
        
        return review_result
        
    except Exception as e:
        print(f"AI å¯©é–±çŸ¥è­˜é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            "overall_assessment": f"AI å¯©é–±éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
            "accuracy_score": 5,
            "clarity_score": 5,
            "teaching_effectiveness": 5,
            "improvement_suggestions": ["è«‹ç¨å¾Œå†è©¦æˆ–è¯çµ¡ç³»çµ±ç®¡ç†å“¡"],
            "potential_confusions": ["ç³»çµ±æš«æ™‚ç„¡æ³•æä¾›åˆ†æ"],
            "recommended_category": knowledge_point_data.get('category', 'N/A'),
            "additional_examples": []
        }
    

def generate_daily_learning_summary(date_str, daily_details, learning_events, model_name=None):
    """
    ä½¿ç”¨AIç”ŸæˆæŒ‡å®šæ—¥æœŸçš„å€‹äººåŒ–å­¸ç¿’ç¸½çµã€‚
    
    Args:
        date_str (str): æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        daily_details (dict): ç•¶æ—¥å­¸ç¿’çµ±è¨ˆæ•¸æ“š
        learning_events (list): ç•¶æ—¥è©³ç´°å­¸ç¿’äº‹ä»¶åˆ—è¡¨
        model_name (str): ä½¿ç”¨çš„AIæ¨¡å‹åç¨±
    
    Returns:
        dict: åŒ…å«AIç¸½çµçš„å®Œæ•´æ•¸æ“š
    """
    
    if not learning_events:
        return {
            "summary": "ä»Šå¤©æ²’æœ‰å­¸ç¿’ç´€éŒ„ã€‚æ˜å¤©æ˜¯æ–°çš„é–‹å§‹ï¼Œä¸€èµ·åŠ æ²¹å§ï¼",
            "key_achievements": [],
            "improvement_suggestions": ["é–‹å§‹æ‚¨çš„ç¬¬ä¸€é“ç¿»è­¯ç·´ç¿’"],
            "motivational_message": "æ¯ä¸€æ¬¡å­¸ç¿’éƒ½æ˜¯é€²æ­¥çš„é–‹å§‹ã€‚"
        }
    
    # æº–å‚™å­¸ç¿’æ•¸æ“šæ‘˜è¦
    total_questions = len(learning_events)
    correct_questions = len([e for e in learning_events if e.get('is_correct')])
    accuracy_rate = (correct_questions / total_questions * 100) if total_questions > 0 else 0
    
    # åˆ†æéŒ¯èª¤æ¨¡å¼
    error_patterns = {}
    common_mistakes = []
    
    for event in learning_events:
        if event.get('ai_feedback') and event['ai_feedback'].get('error_analysis'):
            for error in event['ai_feedback']['error_analysis']:
                error_type = error.get('error_type_code', 'Unknown')
                if error_type not in error_patterns:
                    error_patterns[error_type] = []
                error_patterns[error_type].append(error.get('key_point_summary', ''))
                
                if error.get('severity') == 'major':
                    common_mistakes.append(error.get('key_point_summary', ''))
    
    # æº–å‚™çµ¦AIçš„æç¤º
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è‹±èªå­¸ç¿’é¡§å•å’Œæ•™å­¸å°ˆå®¶ã€‚
    ä½ çš„ä»»å‹™æ˜¯ç‚ºå­¸ç¿’è€…ç”Ÿæˆä¸€ä»½å€‹äººåŒ–ã€æº«æš–ä¸”å…·æœ‰å»ºè¨­æ€§çš„æ¯æ—¥å­¸ç¿’ç¸½çµã€‚
    
    ç¸½çµè¦æ±‚ï¼š
    1. èªæ°£æº«æš–é¼“å‹µï¼Œå°±åƒä¸€ä½é—œå¿ƒå­¸ç”Ÿçš„è€å¸«
    2. é‡é»é—œæ³¨é€²æ­¥å’Œäº®é»ï¼Œå³ä½¿æˆç¸¾ä¸ç†æƒ³ä¹Ÿè¦æ‰¾åˆ°æ­£é¢çš„åœ°æ–¹
    3. æä¾›å…·é«”ã€å¯¦ç”¨çš„æ”¹é€²å»ºè­°
    4. ä½¿ç”¨ç¹é«”ä¸­æ–‡
    5. ä¿æŒå°ˆæ¥­æ€§çš„åŒæ™‚è¦æœ‰äººæ–‡é—œæ‡·
    
    è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
    {
        "summary": "æ•´é«”å­¸ç¿’æƒ…æ³çš„æº«æš–ç¸½çµï¼ˆ150-200å­—ï¼‰",
        "key_achievements": ["ä»Šæ—¥äº®é»1", "ä»Šæ—¥äº®é»2", "ä»Šæ—¥äº®é»3"],
        "improvement_suggestions": ["å…·é«”å»ºè­°1", "å…·é«”å»ºè­°2", "å…·é«”å»ºè­°3"],
        "motivational_message": "æº«æš–çš„æ¿€å‹µè©±èªï¼ˆ50å­—å…§ï¼‰"
    }
    """
    
    user_prompt = f"""
    è«‹ç‚ºä»¥ä¸‹å­¸ç¿’æ•¸æ“šç”Ÿæˆä¸€ä»½æº«æš–çš„æ¯æ—¥ç¸½çµï¼š
    
    ã€åŸºæœ¬æ•¸æ“šã€‘
    æ—¥æœŸï¼š{date_str}
    ç·´ç¿’é¡Œæ•¸ï¼š{total_questions} é¡Œ
    ç­”å°é¡Œæ•¸ï¼š{correct_questions} é¡Œ
    æ­£ç¢ºç‡ï¼š{accuracy_rate:.1f}%
    å­¸ç¿’æ™‚é•·ï¼š{daily_details.get('total_learning_time_seconds', 0)} ç§’
    
    ã€çŸ¥è­˜é»æŒæ¡æƒ…æ³ã€‘
    è¤‡ç¿’çŸ¥è­˜é»ï¼š{len(daily_details.get('reviewed_knowledge_points', []))} å€‹
    æ–°å­¸çŸ¥è­˜é»ï¼š{len(daily_details.get('new_knowledge_points', []))} å€‹
    
    ã€éŒ¯èª¤åˆ†æã€‘
    ä¸»è¦éŒ¯èª¤é¡å‹åˆ†å¸ƒï¼š{dict([(k, len(v)) for k, v in error_patterns.items()])}
    å¸¸è¦‹éŒ¯èª¤çŸ¥è­˜é»ï¼š{common_mistakes[:5]}  # åªé¡¯ç¤ºå‰5å€‹
    
    ã€å­¸ç¿’äº‹ä»¶æ¦‚è¦ã€‘
    ç¸½å…±æœ‰ {len([e for e in learning_events if e.get('question_type') == 'review'])} é“è¤‡ç¿’é¡Œ
    ç¸½å…±æœ‰ {len([e for e in learning_events if e.get('question_type') == 'new'])} é“æ–°é¡Œç›®
    
    è«‹åŸºæ–¼ä»¥ä¸Šæ•¸æ“šï¼Œç”Ÿæˆä¸€ä»½æ—¢å°ˆæ¥­åˆæº«æš–çš„å­¸ç¿’ç¸½çµï¼Œé‡é»é—œæ³¨å­¸ç¿’è€…çš„é€²æ­¥å’Œæˆé•·ã€‚
    """
    
    try:
        summary_result = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " AI æ¯æ—¥ç¸½çµç”Ÿæˆçµæœ " + "="*20)
            print(json.dumps(summary_result, ensure_ascii=False, indent=2))
            print("="*60 + "\n")
        
        # ç¢ºä¿è¿”å›çš„æ•¸æ“šçµæ§‹æ­£ç¢º
        if not isinstance(summary_result, dict):
            raise ValueError("AIè¿”å›çš„æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º")
        
        required_fields = ['summary', 'key_achievements', 'improvement_suggestions', 'motivational_message']
        for field in required_fields:
            if field not in summary_result:
                summary_result[field] = f"AIæœªèƒ½ç”Ÿæˆ{field}å…§å®¹"
        
        return summary_result
        
    except Exception as e:
        print(f"AIç”Ÿæˆæ¯æ—¥ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # è¿”å›å‚™ç”¨ç¸½çµ
        return {
            "summary": f"ä»Šå¤©æ‚¨å®Œæˆäº† {total_questions} é“ç·´ç¿’ï¼Œæ­£ç¢ºç‡ç‚º {accuracy_rate:.1f}%ã€‚æ¯ä¸€æ¬¡çš„ç·´ç¿’éƒ½æ˜¯é€²æ­¥çš„ç©ç´¯ï¼Œç¹¼çºŒä¿æŒé€™æ¨£çš„å­¸ç¿’ç¯€å¥ï¼",
            "key_achievements": [
                f"å®Œæˆäº† {total_questions} é“ç¿»è­¯ç·´ç¿’" if total_questions > 0 else "é–‹å§‹äº†æ–°çš„å­¸ç¿’å˜—è©¦",
                f"æ­£ç¢ºç‡é”åˆ° {accuracy_rate:.1f}%" if accuracy_rate > 0 else "å‹‡æ–¼å˜—è©¦æ–°æŒ‘æˆ°",
                "æŒçºŒæŠ•å…¥è‹±èªå­¸ç¿’"
            ],
            "improvement_suggestions": [
                "å»ºè­°å®šæœŸè¤‡ç¿’éŒ¯èª¤çš„çŸ¥è­˜é»",
                "å¯ä»¥å˜—è©¦å¢åŠ ç·´ç¿’çš„å¤šæ¨£æ€§",
                "ä¿æŒæ¯æ—¥å­¸ç¿’çš„å¥½ç¿’æ…£"
            ],
            "motivational_message": "å­¸ç¿’æ˜¯ä¸€å€‹æŒçºŒçš„éç¨‹ï¼Œæ¯ä¸€æ­¥éƒ½å¾ˆé‡è¦ã€‚ç¹¼çºŒåŠ æ²¹ï¼"
        }
    
def generate_smart_hint(chinese_sentence, user_current_input="", original_hint="", model_name=None):
    """
    ç”ŸæˆAIæ™ºæ…§æç¤º - æ ¹æ“šä½¿ç”¨è€…ç•¶å‰ç¿»è­¯å’Œé¡Œç›®å…§å®¹æä¾›å¼•å°æ€§æç¤º
    
    Args:
        chinese_sentence (str): åŸå§‹ä¸­æ–‡é¡Œç›®
        user_current_input (str): ä½¿ç”¨è€…ç•¶å‰çš„ç¿»è­¯è¼¸å…¥
        original_hint (str): åŸå§‹çš„åŸºæœ¬æç¤º
        model_name (str): ä½¿ç”¨çš„AIæ¨¡å‹åç¨±
    
    Returns:
        dict: åŒ…å«æ™ºæ…§æç¤ºçš„å›æ‡‰
    """
    
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æº«æš–è€Œæœ‰è€å¿ƒçš„è‹±èªå®¶æ•™è€å¸«ã€‚ä½ çš„å°ˆé•·æ˜¯ã€Œå¼•å°å¼æ•™å­¸ã€ï¼Œçµ•å°ä¸æœƒç›´æ¥çµ¦å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯é€éå·§å¦™çš„æå•å’Œæç¤ºï¼Œå•Ÿç™¼å­¸ç”Ÿè‡ªå·±æ€è€ƒå‡ºæ­£ç¢ºçš„ç¿»è­¯ã€‚

    **æ ¸å¿ƒæ•™å­¸åŸå‰‡ï¼š**
    1. ğŸš« çµ•å°ä¸èƒ½ç›´æ¥çµ¦å‡ºå®Œæ•´çš„è‹±æ–‡ç¿»è­¯
    2. ğŸš« ä¸èƒ½ç›´æ¥èªªå‡ºé—œéµå–®å­—æˆ–ç‰‡èªçš„æ­£ç¢ºç­”æ¡ˆ
    3. âœ… è¦å¼•å°å­¸ç”Ÿæ€è€ƒå¥å­çµæ§‹å’Œèªæ³•é‡é»
    4. âœ… è¦é‡å°å­¸ç”Ÿç•¶å‰çš„ç¿»è­¯å…§å®¹çµ¦å‡ºæœ‰é‡å°æ€§çš„å»ºè­°
    5. âœ… è¦ç”¨æº«æš–é¼“å‹µçš„èªæ°£ï¼Œè®“å­¸ç”Ÿæœ‰ä¿¡å¿ƒç¹¼çºŒå˜—è©¦

    **å¼•å°æŠ€å·§ï¼š**
    - ç”¨å•å¥å¼•å°æ€è€ƒï¼šã€Œä½ è¦ºå¾—é€™å€‹å¥å­çš„ä¸»è©æ˜¯ä»€éº¼ï¼Ÿã€
    - æç¤ºå¥å‹çµæ§‹ï¼šã€Œé€™å€‹å¥å­å¯èƒ½éœ€è¦ç”¨åˆ°...çš„å¥å‹ã€
    - åˆ†æ®µå¼•å°ï¼šã€Œæˆ‘å€‘å…ˆè™•ç†å‰åŠå¥ï¼Œå†è™•ç†å¾ŒåŠå¥ã€
    - çµ¦å‡ºæ€è€ƒæ–¹å‘ï¼šã€Œæ³¨æ„é€™è£¡çš„æ™‚æ…‹ã€ã€ã€Œè€ƒæ…®ä¸€ä¸‹èªæ°£ã€
    - æä¾›å°æ¯”ï¼šã€Œä¸­æ–‡èªªæ³•å’Œè‹±æ–‡è¡¨é”ç¿’æ…£æœ‰ä»€éº¼ä¸åŒï¼Ÿã€

    **å›æ‡‰æ ¼å¼ï¼ˆJSONï¼‰ï¼š**
    {
        "smart_hint": "ä½ çš„æ™ºæ…§å¼•å°æç¤ºï¼ˆç¹é«”ä¸­æ–‡ï¼Œ150å­—å…§ï¼‰",
        "thinking_questions": [
            "æ€è€ƒå•é¡Œ1",
            "æ€è€ƒå•é¡Œ2",
            "æ€è€ƒå•é¡Œ3"
        ],
        "encouragement": "é¼“å‹µè©±èªï¼ˆ50å­—å…§ï¼‰"
    }
    """
    
    # æ§‹å»ºä½¿ç”¨è€…æƒ…æ³åˆ†æ
    current_input_analysis = ""
    if user_current_input.strip():
        current_input_analysis = f"""
        **å­¸ç”Ÿç›®å‰çš„ç¿»è­¯å˜—è©¦ï¼š**
        "{user_current_input}"
        
        è«‹æ ¹æ“šå­¸ç”Ÿç›®å‰çš„ç¿»è­¯å…§å®¹ï¼Œåˆ†æä»–å€‘å¯èƒ½é‡åˆ°çš„å›°é›£é»ï¼Œä¸¦é‡å°æ€§åœ°æä¾›å¼•å°ã€‚
        """
    else:
        current_input_analysis = """
        **å­¸ç”Ÿç‹€æ³ï¼š**
        å­¸ç”Ÿç›®å‰é‚„æ²’æœ‰é–‹å§‹ç¿»è­¯ï¼Œæˆ–è€…ç¿»è­¯æ¬„ä½æ˜¯ç©ºçš„ã€‚è«‹æä¾›å…¥é–€ç´šçš„å¼•å°ï¼Œå¹«åŠ©ä»–å€‘é–‹å§‹æ€è€ƒã€‚
        """
    
    # åŸºæœ¬æç¤ºåˆ†æ
    original_hint_analysis = ""
    if original_hint.strip():
        original_hint_analysis = f"""
        **åŸºæœ¬è€ƒé»æç¤ºï¼š**
        "{original_hint}"
        
        é€™æ˜¯é€™é“é¡Œçš„æ ¸å¿ƒæ–‡æ³•è€ƒé»ï¼Œè«‹åœ¨ä¸ç›´æ¥é€éœ²ç­”æ¡ˆçš„å‰æä¸‹ï¼Œå¼•å°å­¸ç”Ÿç†è§£å’Œæ‡‰ç”¨é€™å€‹è€ƒé»ã€‚
        """
    
    user_prompt = f"""
    è«‹ç‚ºä»¥ä¸‹ç¿»è­¯ç·´ç¿’æä¾›æ™ºæ…§å¼•å°ï¼š

    **é¡Œç›®ï¼ˆä¸­æ–‡ï¼‰ï¼š**
    "{chinese_sentence}"

    {current_input_analysis}

    {original_hint_analysis}

    **ä½ çš„ä»»å‹™ï¼š**
    ä½œç‚ºä¸€ä½å°ˆæ¥­çš„è‹±èªå®¶æ•™ï¼Œè«‹æä¾›æº«æš–è€Œæœ‰æ•ˆçš„å¼•å°æç¤ºï¼Œå¹«åŠ©å­¸ç”Ÿè‡ªå·±æ€è€ƒå‡ºæ­£ç¢ºçš„ç¿»è­¯æ–¹å‘ã€‚è¨˜ä½ï¼Œçµ•å°ä¸è¦ç›´æ¥çµ¦å‡ºç­”æ¡ˆï¼Œè€Œæ˜¯è¦å•Ÿç™¼å­¸ç”Ÿçš„æ€è€ƒéç¨‹ã€‚

    è«‹ç‰¹åˆ¥æ³¨æ„ï¼š
    - å¦‚æœå­¸ç”Ÿå·²æœ‰ç¿»è­¯å…§å®¹ï¼Œè¦é‡å°ä»–å€‘çš„å˜—è©¦çµ¦å‡ºå…·é«”çš„æ”¹é€²æ–¹å‘
    - å¦‚æœå­¸ç”Ÿé‚„æ²’é–‹å§‹ï¼Œè¦æä¾›å…¥é–€çš„æ€è€ƒæ¡†æ¶
    - èªæ°£è¦æº«æš–é¼“å‹µï¼Œè®“å­¸ç”Ÿæ„Ÿå—åˆ°è€å¸«çš„è€å¿ƒå’Œé—œæ‡·
    """
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " AI æ™ºæ…§æç¤ºç”Ÿæˆçµæœ " + "="*20)
            print(json.dumps(response_data, ensure_ascii=False, indent=2))
            print("="*60 + "\n")
        
        # ç¢ºä¿å›æ‡‰æ ¼å¼æ­£ç¢º
        if not isinstance(response_data, dict):
            raise ValueError("AIè¿”å›çš„æ ¼å¼ä¸æ­£ç¢º")
        
        required_fields = ['smart_hint', 'thinking_questions', 'encouragement']
        for field in required_fields:
            if field not in response_data:
                response_data[field] = "AIç”Ÿæˆå…§å®¹ä¸å®Œæ•´"
        
        # ç¢ºä¿ thinking_questions æ˜¯åˆ—è¡¨
        if not isinstance(response_data['thinking_questions'], list):
            response_data['thinking_questions'] = ["è«‹å˜—è©¦åˆ†æå¥å­çµæ§‹", "è€ƒæ…®ä¸€ä¸‹æ™‚æ…‹å’Œèªæ…‹", "æ³¨æ„ä¸­è‹±æ–‡è¡¨é”çš„å·®ç•°"]
        
        return response_data
        
    except Exception as e:
        print(f"AIç”Ÿæˆæ™ºæ…§æç¤ºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # å›å‚³å‚™ç”¨æç¤º
        return {
            "smart_hint": "è®“æˆ‘å€‘ä¸€æ­¥æ­¥ä¾†åˆ†æé€™å€‹å¥å­å§ï¼é¦–å…ˆï¼Œä½ èƒ½æ‰¾å‡ºé€™å€‹å¥å­çš„ä¸»è©å’Œå‹•è©å—ï¼Ÿç„¶å¾Œæ€è€ƒä¸€ä¸‹é€™å€‹å¥å­çš„æ™‚æ…‹å’Œèªæ°£ã€‚",
            "thinking_questions": [
                "é€™å€‹å¥å­çš„ä¸»è©æ˜¯ä»€éº¼ï¼Ÿ",
                "å‹•è©ç”¨ä»€éº¼æ™‚æ…‹æ¯”è¼ƒåˆé©ï¼Ÿ",
                "æœ‰æ²’æœ‰éœ€è¦æ³¨æ„çš„ç‰¹æ®Šå¥å‹ï¼Ÿ"
            ],
            "encouragement": "æ…¢æ…¢ä¾†ï¼Œæ¯ä¸€æ¬¡æ€è€ƒéƒ½æ˜¯é€²æ­¥ï¼"
        }
    
def generate_vocabulary_definition(word, context=None, model_name=None):
    """ä½¿ç”¨LLMç‚ºå–®å­—ç”Ÿæˆå®Œæ•´çš„å­¸ç¿’è³‡è¨Š"""
    
    system_prompt = f"""
    ä½ æ˜¯å°ˆæ¥­çš„è‹±èªè©å…¸ç·¨è¼¯å’Œèªè¨€å­¸ç¿’å°ˆå®¶ï¼Œè«‹ç‚ºå–®å­— "{word}" æä¾›æº–ç¢ºä¸”é©åˆå°ç£å­¸ç¿’è€…çš„å®Œæ•´å­¸ç¿’è³‡è¨Šã€‚

    å›å‚³æ ¼å¼è¦æ±‚ï¼š
    ä½ çš„å›è¦†ã€Œå¿…é ˆã€æ˜¯ä¸€å€‹ JSON ç‰©ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
    
    {{
        "word": "{word}",
        "pronunciation_ipa": "åœ‹éš›éŸ³æ¨™ï¼ˆIPAæ ¼å¼ï¼‰",
        "part_of_speech": "è©æ€§ï¼ˆåè©/å‹•è©/å½¢å®¹è©ç­‰ï¼‰",
        "definition_zh": "æ¸…æ™°çš„ç¹é«”ä¸­æ–‡å®šç¾©ï¼Œé©åˆå°ç£å­¸ç¿’è€…ç†è§£",
        "definition_en": "ç°¡æ½”çš„è‹±æ–‡å®šç¾©",
        "difficulty_level": 1-5çš„æ•´æ•¸ï¼ˆ1=åœ‹ä¸­ç¨‹åº¦ï¼Œ5=é«˜éšå­¸è¡“ï¼‰ï¼Œ
        "word_frequency_rank": ä¼°è¨ˆçš„è©é »æ’åï¼ˆ1-10000ï¼‰ï¼Œ
        "example_sentences": [
            {{"en": "å¯¦ç”¨çš„è‹±æ–‡ä¾‹å¥", "zh": "å°æ‡‰çš„ä¸­æ–‡ç¿»è­¯"}},
            {{"en": "å¦ä¸€å€‹ä¾‹å¥", "zh": "ä¸­æ–‡ç¿»è­¯"}}
        ],
        "word_family": ["ç›¸é—œçš„è©å½™è®ŠåŒ–"],
        "common_collocations": ["å¸¸ç”¨çš„æ­é…è©çµ„"],
        "synonyms": ["åŒç¾©è©"],
        "antonyms": ["åç¾©è©"]
    }}
    
    æ³¨æ„äº‹é …ï¼š
    1. éŸ³æ¨™ä½¿ç”¨æ¨™æº–IPAæ ¼å¼
    2. ä¸­æ–‡å®šç¾©è¦æº–ç¢ºä¸”å®¹æ˜“ç†è§£
    3. ä¾‹å¥è¦å¯¦ç”¨ä¸”ä¸è¦å¤ªè¤‡é›œ
    4. é›£åº¦è©•ä¼°è¦è€ƒæ…®å°ç£å­¸ç¿’è€…çš„ç¨‹åº¦
    """
    
    user_prompt = f"è«‹ç‚ºå–®å­— '{word}' ç”Ÿæˆå®Œæ•´çš„å­¸ç¿’è³‡æ–™ã€‚"
    if context:
        user_prompt += f"\n\nèªå¢ƒåƒè€ƒï¼š{context}\nè«‹ç‰¹åˆ¥è€ƒæ…®é€™å€‹èªå¢ƒä¸­çš„ç”¨æ³•ã€‚"
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " å–®å­—å®šç¾©ç”Ÿæˆçµæœ " + "="*20)
            print(json.dumps(response_data, ensure_ascii=False, indent=2))
            print("="*60 + "\n")
        
        # é©—è­‰å¿…è¦æ¬„ä½
        required_fields = ['word', 'definition_zh', 'part_of_speech']
        for field in required_fields:
            if field not in response_data:
                raise ValueError(f"AIå›æ‡‰ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{field}")
        
        # è¨­å®šé è¨­å€¼
        response_data.setdefault('difficulty_level', 3)
        response_data.setdefault('word_frequency_rank', 5000)
        response_data.setdefault('example_sentences', [])
        response_data.setdefault('word_family', [])
        response_data.setdefault('common_collocations', [])
        response_data.setdefault('synonyms', [])
        response_data.setdefault('antonyms', [])
        
        return response_data
        
    except Exception as e:
        print(f"AIç”Ÿæˆå–®å­—å®šç¾©æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # å›å‚³åŸºæœ¬çš„å‚™ç”¨è³‡æ–™
        return {
            "word": word,
            "pronunciation_ipa": "/wÉœËrd/",
            "part_of_speech": "unknown",
            "definition_zh": f"å–®å­— '{word}' çš„å®šç¾©ï¼ˆAIç”Ÿæˆå¤±æ•—ï¼‰",
            "definition_en": f"Definition of '{word}' (AI generation failed)",
            "difficulty_level": 3,
            "word_frequency_rank": 5000,
            "example_sentences": [],
            "word_family": [],
            "common_collocations": [],
            "synonyms": [],
            "antonyms": []
        }

def generate_vocabulary_quiz_options(target_word, correct_definition, question_type="multiple_choice", model_name=None):
    """ç‚ºå–®å­—æ¸¬é©—ç”Ÿæˆé¸æ“‡é¡Œé¸é …"""
    
    system_prompt = f"""
    ä½ æ˜¯å°ˆæ¥­çš„è‹±èªæ¸¬é©—é¡Œç›®è¨­è¨ˆå¸«ï¼Œéœ€è¦ç‚ºå–®å­— "{target_word}" çš„æ¸¬é©—ç”Ÿæˆé«˜å“è³ªçš„å¹²æ“¾é¸é …ã€‚
    
    è¨­è¨ˆåŸå‰‡ï¼š
    1. å¹²æ“¾é¸é …è¦æœ‰ä¸€å®šçš„è¿·æƒ‘æ€§ï¼Œä½†ä¸èƒ½å®Œå…¨ä¸åˆç†
    2. é¸é …æ‡‰è©²æ˜¯ç›¸ä¼¼ç¨‹åº¦çš„è©å½™æˆ–æ¦‚å¿µ
    3. é¿å…éæ–¼æ˜é¡¯çš„éŒ¯èª¤ç­”æ¡ˆ
    4. è€ƒæ…®å°ç£å­¸ç¿’è€…å®¹æ˜“æ··æ·†çš„åœ°æ–¹
    
    å›å‚³æ ¼å¼ï¼š
    {{
        "question_text": "è«‹é¸æ“‡ '{target_word}' çš„æ­£ç¢ºä¸­æ–‡æ„æ€",
        "correct_answer": "{correct_definition}",
        "wrong_options": [
            "å¹²æ“¾é¸é …1",
            "å¹²æ“¾é¸é …2", 
            "å¹²æ“¾é¸é …3"
        ],
        "explanation": "ç‚ºä»€éº¼æ­£ç¢ºç­”æ¡ˆæ˜¯å°çš„ï¼Œä»¥åŠå¹²æ“¾é¸é …ç‚ºä»€éº¼éŒ¯èª¤çš„ç°¡çŸ­èªªæ˜"
    }}
    """
    
    user_prompt = f"""
    ç›®æ¨™å–®å­—ï¼š{target_word}
    æ­£ç¢ºå®šç¾©ï¼š{correct_definition}
    é¡Œç›®é¡å‹ï¼š{question_type}
    
    è«‹ç”Ÿæˆ3å€‹åˆé©çš„å¹²æ“¾é¸é …ï¼Œè®“é€™é“é¡Œç›®æœ‰é©ç•¶çš„é‘‘åˆ¥åº¦ã€‚
    """
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " å–®å­—æ¸¬é©—é¸é …ç”Ÿæˆçµæœ " + "="*20)
            print(json.dumps(response_data, ensure_ascii=False, indent=2))
            print("="*60 + "\n")
        
        # é©—è­‰æ ¼å¼
        if not isinstance(response_data.get('wrong_options'), list) or len(response_data['wrong_options']) != 3:
            raise ValueError("å¹²æ“¾é¸é …æ ¼å¼ä¸æ­£ç¢º")
        
        return response_data
        
    except Exception as e:
        print(f"AIç”Ÿæˆæ¸¬é©—é¸é …æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # å›å‚³åŸºæœ¬çš„å‚™ç”¨é¸é …
        return {
            "question_text": f"è«‹é¸æ“‡ '{target_word}' çš„æ­£ç¢ºä¸­æ–‡æ„æ€",
            "correct_answer": correct_definition,
            "wrong_options": [
                "é¸é …Aï¼ˆAIç”Ÿæˆå¤±æ•—ï¼‰",
                "é¸é …Bï¼ˆAIç”Ÿæˆå¤±æ•—ï¼‰", 
                "é¸é …Cï¼ˆAIç”Ÿæˆå¤±æ•—ï¼‰"
            ],
            "explanation": "æ¸¬é©—é¸é …ç”Ÿæˆå¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥ã€‚"
        }

def analyze_word_difficulty(word, user_level="intermediate", model_name=None):
    """AIåˆ†æå–®å­—é›£åº¦ç­‰ç´š"""
    
    system_prompt = f"""
    ä½ æ˜¯è‹±èªæ•™å­¸å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–ç‚ºè‹±æ–‡å–®å­—è©•ä¼°é›£åº¦ç­‰ç´šï¼ˆ1-5ï¼‰ï¼š
    
    é›£åº¦åˆ†ç´šæ¨™æº–ï¼š
    1ç´šï¼šåœ‹ä¸­åŸºç¤è©å½™ (cat, run, happy, book)
    2ç´šï¼šé«˜ä¸­å¸¸ç”¨è©å½™ (environment, successful, describe)  
    3ç´šï¼šå¤§å­¸å…¥å­¸è©å½™ (sophisticated, analyze, comprehensive)
    4ç´šï¼šé«˜éšå­¸è¡“è©å½™ (methodology, paradigm, correlation)
    5ç´šï¼šå°ˆæ¥­/ç½•è¦‹è©å½™ (ubiquitous, ephemeral, serendipity)
    
    è©•ä¼°è€ƒé‡å› ç´ ï¼š
    - è©å½™åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä½¿ç”¨é »ç‡
    - éŸ³ç¯€è¤‡é›œåº¦å’Œæ‹¼å¯«é›£åº¦
    - è©æ ¹è©ç¶´çš„è¤‡é›œç¨‹åº¦
    - èªç¾©çš„æŠ½è±¡ç¨‹åº¦
    - åœ¨å°ç£è‹±èªæ•™è‚²ä¸­çš„å‡ºç¾æ™‚æ©Ÿ
    
    ä½¿ç”¨è€…ç¨‹åº¦åƒè€ƒï¼š{user_level}
    
    å›å‚³æ ¼å¼ï¼š
    {{
        "difficulty_level": 1-5çš„æ•´æ•¸,
        "frequency_estimate": 1-10000çš„è©é »ä¼°è¨ˆæ’å,
        "reasoning": "é›£åº¦è©•ä¼°çš„ç†ç”±èªªæ˜",
        "learning_tips": "å­¸ç¿’é€™å€‹å–®å­—çš„å»ºè­°æ–¹æ³•"
    }}
    """
    
    user_prompt = f"è«‹è©•ä¼°å–®å­— '{word}' çš„é›£åº¦ç­‰ç´šå’Œå­¸ç¿’å»ºè­°ã€‚"
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        # ç¢ºä¿é›£åº¦ç­‰ç´šåœ¨æœ‰æ•ˆç¯„åœå…§
        difficulty = response_data.get('difficulty_level', 3)
        if not isinstance(difficulty, int) or difficulty < 1 or difficulty > 5:
            difficulty = 3
        
        frequency = response_data.get('frequency_estimate', 5000)
        if not isinstance(frequency, int) or frequency < 1:
            frequency = 5000
        
        return {
            'difficulty_level': difficulty,
            'frequency_estimate': frequency,
            'reasoning': response_data.get('reasoning', 'é›£åº¦åˆ†æè™•ç†ä¸­'),
            'learning_tips': response_data.get('learning_tips', 'å»ºè­°å¤šç·´ç¿’ä½¿ç”¨')
        }
        
    except Exception as e:
        print(f"AIåˆ†æå–®å­—é›£åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            'difficulty_level': 3,
            'frequency_estimate': 5000,
            'reasoning': 'AIåˆ†æå¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼',
            'learning_tips': 'å»ºè­°æŸ¥é–±å­—å…¸ä¸¦å¤šåŠ ç·´ç¿’'
        }

def extract_vocabulary_from_translation_error(knowledge_point_data, model_name=None):
    """å¾ç¿»è­¯éŒ¯èª¤ä¸­æ™ºæ…§æå–æ‡‰è©²å­¸ç¿’çš„å–®å­—"""
    
    system_prompt = """
    ä½ æ˜¯è‹±èªå­¸ç¿’åˆ†æå°ˆå®¶ï¼Œéœ€è¦å¾å­¸ç”Ÿçš„ç¿»è­¯éŒ¯èª¤ä¸­æå–å‡ºã€ŒçœŸæ­£å€¼å¾—å­¸ç¿’çš„æ ¸å¿ƒå–®å­—ã€ã€‚
    
    åˆ†æåŸå‰‡ï¼š
    1. å°ˆæ³¨æ–¼ã€Œè©å½™é¡éŒ¯èª¤ã€ï¼Œå¿½ç•¥ç´”èªæ³•éŒ¯èª¤
    2. æå–å­¸ç”Ÿä¸ç†Ÿæ‚‰æˆ–ç”¨éŒ¯çš„ã€Œé—œéµå–®å­—ã€
    3. å„ªå…ˆè€ƒæ…®å¯¦ç”¨æ€§é«˜çš„è©å½™
    4. é¿å…éæ–¼åŸºç¤æˆ–éæ–¼å†·åƒ»çš„å–®å­—
    
    å›å‚³æ ¼å¼ï¼š
    {{
        "extracted_words": [
            {{
                "word": "ç›®æ¨™å–®å­—",
                "reason": "ç‚ºä»€éº¼é€™å€‹å–®å­—å€¼å¾—å­¸ç¿’",
                "difficulty_estimate": 1-5,
                "priority": "high/medium/low"
            }}
        ],
        "analysis_summary": "æ•´é«”åˆ†æèªªæ˜"
    }}
    
    å¦‚æœæ²’æœ‰æ‰¾åˆ°åˆé©çš„å–®å­—ï¼Œå›å‚³ç©ºé™£åˆ—ã€‚
    """
    
    user_prompt = f"""
    è«‹åˆ†æä»¥ä¸‹ç¿»è­¯éŒ¯èª¤ï¼Œæå–å€¼å¾—å­¸ç¿’çš„å–®å­—ï¼š
    
    éŒ¯èª¤åˆ†é¡ï¼š{knowledge_point_data.get('category', 'æœªçŸ¥')}
    éŒ¯èª¤å­åˆ†é¡ï¼š{knowledge_point_data.get('subcategory', 'æœªçŸ¥')}
    æ­£ç¢ºè¡¨é”ï¼š{knowledge_point_data.get('correct_phrase', 'æœªçŸ¥')}
    å­¸ç”ŸåŸå¥ï¼š{knowledge_point_data.get('user_context_sentence', 'æœªçŸ¥')}
    éŒ¯èª¤ç‰‡æ®µï¼š{knowledge_point_data.get('incorrect_phrase_in_context', 'æœªçŸ¥')}
    
    è«‹å¾ä¸­æå–æ‡‰è©²åŠ å…¥å–®å­—åº«çš„è©å½™ã€‚
    """
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        if MONITOR_MODE:
            print("\n" + "="*20 + " ç¿»è­¯éŒ¯èª¤å–®å­—æå–çµæœ " + "="*20)
            print(json.dumps(response_data, ensure_ascii=False, indent=2))
            print("="*60 + "\n")
        
        extracted_words = response_data.get('extracted_words', [])
        
        # éæ¿¾å’Œé©—è­‰æå–çš„å–®å­—
        valid_words = []
        for word_info in extracted_words:
            if isinstance(word_info, dict) and 'word' in word_info:
                # åŸºæœ¬çš„å–®å­—é©—è­‰ï¼ˆåªåŒ…å«å­—æ¯ï¼‰
                word = word_info['word'].strip().lower()
                if word.isalpha() and len(word) > 1:
                    valid_words.append({
                        'word': word,
                        'reason': word_info.get('reason', 'ç¿»è­¯éŒ¯èª¤ä¸­ç™¼ç¾'),
                        'difficulty_estimate': word_info.get('difficulty_estimate', 3),
                        'priority': word_info.get('priority', 'medium')
                    })
        
        return {
            'extracted_words': valid_words,
            'analysis_summary': response_data.get('analysis_summary', 'åˆ†æå®Œæˆ')
        }
        
    except Exception as e:
        print(f"AIæå–ç¿»è­¯éŒ¯èª¤å–®å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            'extracted_words': [],
            'analysis_summary': 'æå–å¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥'
        }

def generate_context_fill_question(word, difficulty_level=3, model_name=None):
    """ç‚ºæŒ‡å®šå–®å­—ç”Ÿæˆèªå¢ƒå¡«ç©ºé¡Œ"""
    
    system_prompt = f"""
    ä½ æ˜¯è‹±èªæ¸¬é©—è¨­è¨ˆå°ˆå®¶ï¼Œéœ€è¦ç‚ºå–®å­— "{word}" è¨­è¨ˆä¸€é“èªå¢ƒå¡«ç©ºé¡Œã€‚
    
    è¨­è¨ˆè¦æ±‚ï¼š
    1. å‰µé€ ä¸€å€‹è‡ªç„¶çš„è‹±æ–‡å¥å­ï¼Œå…¶ä¸­åŒ…å«ç›®æ¨™å–®å­—
    2. å¥å­è¦æœ‰è¶³å¤ çš„èªå¢ƒç·šç´¢ï¼Œä½†ä¸èƒ½éæ–¼æ˜é¡¯
    3. é›£åº¦ç­‰ç´šï¼š{difficulty_level}/5
    4. å¥å­é•·åº¦é©ä¸­ï¼Œé©åˆå°ç£å­¸ç¿’è€…
    
    å›å‚³æ ¼å¼ï¼š
    {{
        "question_sentence": "åŒ…å«ç©ºæ ¼_____çš„å¥å­",
        "complete_sentence": "å®Œæ•´çš„å¥å­",
        "target_word": "{word}",
        "context_hints": ["èªå¢ƒæç¤º1", "èªå¢ƒæç¤º2"],
        "difficulty_level": {difficulty_level}
    }}
    """
    
    user_prompt = f"è«‹ç‚ºå–®å­— '{word}' è¨­è¨ˆä¸€é“èªå¢ƒå¡«ç©ºé¡Œï¼Œé›£åº¦ç­‰ç´šç‚º {difficulty_level}ã€‚"
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        
        # é©—è­‰å¿…è¦æ¬„ä½
        if 'question_sentence' not in response_data or 'complete_sentence' not in response_data:
            raise ValueError("èªå¢ƒå¡«ç©ºé¡Œæ ¼å¼ä¸å®Œæ•´")
        
        return response_data
        
    except Exception as e:
        print(f"AIç”Ÿæˆèªå¢ƒå¡«ç©ºé¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            "question_sentence": f"The _____ was very important in this situation.",
            "complete_sentence": f"The {word} was very important in this situation.",
            "target_word": word,
            "context_hints": ["æ ¹æ“šèªå¢ƒåˆ¤æ–·"],
            "difficulty_level": difficulty_level
        }

def batch_enhance_vocabulary_definitions(word_list, model_name=None):
    """æ‰¹æ¬¡å¢å¼·å–®å­—å®šç¾©ï¼ˆç”¨æ–¼æ”¹å–„ç¾æœ‰å–®å­—åº«ï¼‰"""
    
    system_prompt = """
    ä½ æ˜¯å°ˆæ¥­çš„è©å…¸ç·¨è¼¯ï¼Œéœ€è¦æ‰¹æ¬¡è™•ç†å¤šå€‹è‹±æ–‡å–®å­—ï¼Œç‚ºæ¯å€‹å–®å­—æä¾›å¢å¼·çš„å­¸ç¿’è³‡è¨Šã€‚
    
    å°æ–¼æ¯å€‹å–®å­—ï¼Œè«‹æä¾›ï¼š
    1. æ›´æº–ç¢ºçš„ä¸­æ–‡å®šç¾©
    2. å¯¦ç”¨çš„ä¾‹å¥
    3. å¸¸ç”¨æ­é…
    
    å›å‚³æ ¼å¼ï¼š
    {{
        "enhanced_definitions": [
            {{
                "word": "å–®å­—",
                "improved_definition_zh": "æ”¹å–„çš„ä¸­æ–‡å®šç¾©",
                "example_sentences": [
                    {{"en": "è‹±æ–‡ä¾‹å¥", "zh": "ä¸­æ–‡ç¿»è­¯"}}
                ],
                "collocations": ["å¸¸ç”¨æ­é…"]
            }}
        ]
    }}
    """
    
    word_list_str = ", ".join(word_list)
    user_prompt = f"è«‹ç‚ºä»¥ä¸‹å–®å­—æä¾›å¢å¼·çš„å®šç¾©å’Œå­¸ç¿’è³‡è¨Šï¼š{word_list_str}"
    
    try:
        response_data = _call_llm_api(system_prompt, user_prompt, model_name, DEFAULT_GENERATION_MODEL)
        return response_data.get('enhanced_definitions', [])
        
    except Exception as e:
        print(f"AIæ‰¹æ¬¡å¢å¼·å–®å­—å®šç¾©æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

# å­—å…¸APIæ•´åˆåŠŸèƒ½

def fetch_cambridge_definition(word):
    """èª¿ç”¨åŠæ©‹å­—å…¸APIï¼ˆæˆ–å‚™ç”¨å…è²»APIï¼‰"""
    try:
        import requests
    except ImportError:
        print("è­¦å‘Š: requests å¥—ä»¶æœªå®‰è£ï¼Œå­—å…¸APIåŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        return None
    
    # é¦–å…ˆå˜—è©¦å…è²»çš„å­—å…¸APIï¼ˆå› ç‚ºåŠæ©‹å­—å…¸APIéœ€è¦ä»˜è²»ï¼‰
    try:
        free_api_url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
        response = requests.get(free_api_url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if data and len(data) > 0:
                entry = data[0]
                
                # æå–è³‡è¨Š
                pronunciation = ""
                if 'phonetics' in entry:
                    for phonetic in entry['phonetics']:
                        if 'text' in phonetic:
                            pronunciation = phonetic['text']
                            break
                
                definitions = []
                examples = []
                part_of_speech = ""
                
                if 'meanings' in entry:
                    for meaning in entry['meanings']:
                        if not part_of_speech:
                            part_of_speech = meaning.get('partOfSpeech', '')
                        
                        for definition in meaning.get('definitions', []):
                            definitions.append(definition.get('definition', ''))
                            if 'example' in definition:
                                examples.append(definition['example'])
                
                return {
                    'word': word,
                    'pronunciation_ipa': pronunciation,
                    'part_of_speech': part_of_speech,
                    'definition_en': '; '.join(definitions[:2]),  # å–å‰å…©å€‹å®šç¾©
                    'examples': examples[:2],  # å–å‰å…©å€‹ä¾‹å¥
                    'source': 'free_dictionary_api'
                }
    
    except Exception as e:
        print(f"å­—å…¸APIæŸ¥è©¢å¤±æ•—: {e}")
    
    # å¦‚æœAPIå¤±æ•—ï¼Œå›å‚³Noneï¼Œè®“ç³»çµ±ä½¿ç”¨LLMç”Ÿæˆ
    return None

def smart_add_vocabulary_word(word, context=None, source_type='manual', source_reference_id=None):
    """æ™ºæ…§æ–°å¢å–®å­—ï¼šçµåˆå­—å…¸APIå’ŒLLM"""
    
    print(f"æ­£åœ¨æ™ºæ…§åˆ†æå–®å­—: {word}")
    
    # 1. é¦–å…ˆå˜—è©¦å­—å…¸API
    dict_data = fetch_cambridge_definition(word)
    
    # 2. ä½¿ç”¨LLMç”Ÿæˆå®Œæ•´å®šç¾©ï¼ˆåŒ…å«ä¸­æ–‡ï¼‰
    llm_data = generate_vocabulary_definition(word, context)
    
    # 3. åˆä½µæœ€ä½³è³‡è¨Š
    final_data = {
        'word': word.lower().strip(),
        'source_type': source_type,
        'source_reference_id': source_reference_id,
        'added_context': context
    }
    
    if dict_data:
        # å„ªå…ˆä½¿ç”¨å­—å…¸APIçš„éŸ³æ¨™å’Œè‹±æ–‡å®šç¾©
        final_data.update({
            'pronunciation_ipa': dict_data.get('pronunciation_ipa') or llm_data.get('pronunciation_ipa'),
            'part_of_speech': dict_data.get('part_of_speech') or llm_data.get('part_of_speech'),
            'definition_en': dict_data.get('definition_en') or llm_data.get('definition_en'),
            'definition_zh': llm_data.get('definition_zh'),  # LLMçš„ä¸­æ–‡å®šç¾©
            'difficulty_level': llm_data.get('difficulty_level', 3),
            'word_frequency_rank': llm_data.get('word_frequency_rank', 5000)
        })
        
        # åˆä½µä¾‹å¥
        dict_examples = [{'sentence_en': ex, 'source': 'dictionary'} for ex in dict_data.get('examples', [])]
        llm_examples = [{'sentence_en': ex['en'], 'sentence_zh': ex.get('zh'), 'source': 'llm'} 
                       for ex in llm_data.get('example_sentences', [])]
        final_data['examples'] = dict_examples + llm_examples
        
    else:
        # å®Œå…¨ä½¿ç”¨LLMè³‡æ–™
        final_data.update({
            'pronunciation_ipa': llm_data.get('pronunciation_ipa'),
            'part_of_speech': llm_data.get('part_of_speech'),
            'definition_en': llm_data.get('definition_en'),
            'definition_zh': llm_data.get('definition_zh'),
            'difficulty_level': llm_data.get('difficulty_level', 3),
            'word_frequency_rank': llm_data.get('word_frequency_rank', 5000),
            'examples': [{'sentence_en': ex['en'], 'sentence_zh': ex.get('zh'), 'source': 'llm'} 
                        for ex in llm_data.get('example_sentences', [])]
        })
    
    return final_data